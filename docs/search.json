[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "DA.html",
    "href": "DA.html",
    "title": "DeepAcoustics Guide and Tutorial",
    "section": "",
    "text": "DeepAcoustics is an open-access, MATLAB-based tool that was modified from a program called DeepSqueak (Coffey et al., 2019). Originally designed for use with ultrasonic calls from rats for neurological studies, our modification of the program has led to an independent version of the software that is optimized to detect calls in noisier, underwater environments. Additional modifications to the classification capabilities are incorporated in DeepAcoustics.\nModifications to DeepAcoustics allowed for a broadened scope that prioritizes underwater acoustic environments and species, while still accommodating in-air terrestrial counterparts. These enhancements extend the tool’s functionality, with key improvements including:\n\nAccess to wider bandwidth, including low frequencies that were not previously required for DeepSqueak.\nIncreased user interactivity in image creation, network training, and multiclass sound detection—ideal for analyzing overlapping call types across bandwidths.\nSupport for custom duration audio file sizes and integration with Raven selection tables, often used for marine call annotation.\nProcessing of large acoustic datasets, with the ability to extract date and time stamps directly from file names for streamlined data management.\nOngoing advancements in classification capabilities, ensuring the tool stays at the forefront of bioacoustic research.\n\nThese modifications build on the intuitive graphical user interface (GUI) originally developed by Coffey et al., ensuring DeepAcoustics remains accessible to users with varying levels of bioacoustic expertise. It provides a powerful, flexible deep learning tool that simplifies network training and deployment, empowering researchers to tackle both marine and terrestrial acoustic challenges efficiently.\nFor access to this software, please visit the DeepAcoustics repository.\n\n\nSoftware Requirements:\nDeepAcoustics is developed in MATLAB and utilizes the streamlined integration of deep learning modules along with the YOLO (“You Only Look Once”) object detection model, specifically version 4. While this version is not as advanced as Python’s YOLO v9, it offers robust performance for bioacoustic applications. To run DeepAcoustics, the following are required:\n\nMATLAB version R2023b or higher\nDeep Learning Toolbox\nComputer Vision Toolbox\nSignal Processing Toolbox\nParallel Computing Toolbox (for GPU support)\nStatistics and Machine Learning Toolbox\n\nThese requirements ensure optimal performance and efficient handling of deep learning tasks within the software.\n\n\n\nGPU Requirements:\nA GPU (Graphics Processing Unit) is a specialized hardware component designed to handle complex mathematical calculations, especially those related to graphics rendering and parallel processing tasks. While GPUs were originally developed for rendering images and videos in gaming and 3D applications, their architecture has made them highly effective for tasks that involve large-scale matrix and vector operations, which are common in deep learning.\nDeep learning tasks, such as training neural networks, require the processing of vast amounts of data and the performance of many computations simultaneously. These tasks involve operations like multiplying large matrices (weights and inputs), calculating gradients, and updating model parameters across multiple layers of the network. CPUs (Central Processing Units) can handle these tasks but are optimized for sequential processing, meaning they are generally slower when performing the parallelized operations deep learning demands. GPUs, on the other hand, are designed to process thousands of operations in parallel, making them much faster for deep learning tasks. This capability is essential when working with large datasets and complex models, such as convolutional neural networks (CNNs) or object detection models like YOLO, where speed and efficiency are crucial. By using GPUs, deep learning models can be trained faster and more efficiently, significantly reducing the time needed for both training and inference, particularly when working with large-scale or real-time applications.\nA modern GPU with CUDA support is required for training networks in DeepAcoustics, but is not required for using a trained network model to detect target calls in audio using DeepAcoustics.\n\n\n\nGetting Started\nThis documentation takes users through the features of DeepAcoustics, and then includes a tutorial that users can use for further exploration of this deep learning GUI. Please begin by downloading DeepAcoustics from the GitHub repository below. Then open MATLAB, set the path to the folder you downloaded the software to, and use the word “DeepAcoustics” to call the tool. We recommend using GitHub Desktop to streamline the incorporation of updates to the software.\nAccess DeepAcoustics\n\n\n\nHow to Get Help\n\nSoftware Issues\nIf you encounter any issues or have questions about DeepAcoustics, please visit the DeepAcoustics Software Issues section. To help us resolve problems efficiently, include the following details when reporting an issue:\n\nMATLAB version you’re using\nPlatform and OS version (e.g., Windows 11, macOS Ventura)\nSteps or code that triggered the issue\nWhat happened vs. what you expected\nAny error messages or unusual behavior\nApply the appropriate description in the title (e.g., “Training Bug”)\n\n\n\nDiscussions\nFor general inquiries, ideas, or feedback that don’t fit into an issue report, use the Discussions section. This space is ideal for:\n\nSharing ideas or suggestions for new features\nDiscussing best practices for using the software effectively\nEngaging with other users to ask questions or share insights\n\nBoth sections are monitored to ensure you receive timely support and can connect with the broader DeepAcoustics community.\n\nDevelopment: DeepAcoustics is a joint accessible tool venture development lead by Acoustic Interactions and supported by Ocean Science Analytics.\nSoftware Developers: Gabi Alongi, Peter Sugarman, Liz Ferguson, Jennifer Pettis Schallert, and original DeepSqueak Development by Ruby Marx, Kevin Coffey, Robert Ciszek, & Leonardo Lara-Valderrábano\n© Tutorial Copyright 2024 Ocean Science Analytics\nBuilt with Quarto using modified Lux theme",
    "crumbs": [
      "About DeepAcoustics"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "BioAcoustics & Deep Learning",
    "section": "",
    "text": "Deep Learning Methods\nRecent developments in deep learning approaches to detection and classification of signals from some species have proven successful at accurately detecting calls using trained models in varying noise conditions. Deep learning methods of detection and classification are increasing in utility within the underwater acoustic community (Allen et al., 2021; Shui et al., 2020; Vickers et al., 2021). Deep neural networks use automated means of spectrogram image classification that perform well when using an appropriately trained model. By means of rapid automated detection independent of operator experience, deep learning methods have the potential to minimize the drudgery of processing passive acoustic data, leading to more timely use of this information in research studies or biodiversity monitoring projects.\nA variety of signal processing, pattern recognition and machine learning techniques have improved detection and classification of marine mammal sounds through automation, but vary with respect to performance level (Usman, et al., 2020). Deep learning models are a form of machine learning that applies different filter banks at different scales and determines features used to discriminate signals during a learning stage (Bianco, et al., 2019). The models are thus not reliant on meeting criteria for a series of target values, but rather independently determine important features using one of several neural networks. Several studies report high percentages of precision and recall, and improved performance when testing involved multiple datasets collected in variable acoustic conditions (Kirsebom, et al., 2020; Shiu et al., 2020). In addition to improved detection, deep neural networks offer capabilities in classifying marine mammal vocalizations, allowing for their use with multi-species analyses (Thomas et al., 2019).\n\nAllen, A. N., Harvey, M., Harrell, L., Jansen, A., Merkens, K. P., Wall, C. C., … & Oleson, E. M. (2021). A convolutional neural network for automated detection of humpback whale song in a diverse, long-term passive acoustic dataset. Frontiers in Marine Science, 8, 165.\nBianco, M. J., Gerstoft, P., Traer, J., Ozanich, E., Roch, M. A., Gannot, S., & Deledalle, C. A. (2019). Machine learning in acoustics: Theory and applications. The Journal of the Acoustical Society of America, 146(5), 3590-3628.\nCoffey, K. R., Marx, R. E., & Neumaier, J. F. (2019). DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations. Neuropsychopharmacology, 44(5), 859-868.\nKirsebom, O. S., Frazao, F., Simard, Y., Roy, N., Matwin, S., & Giard, S. (2020). Performance of a deep neural network at detecting North Atlantic right whale upcalls. The Journal of the Acoustical Society of America, 147(4), 2636-2646.\nShiu, Y., Palmer, K. J., Roch, M. A., Fleishman, E., Liu, X., Nosal, E. M., … & Klinck, H. (2020). Deep neural networks for automated detection of marine mammal species. Scientific reports, 10(1), 1-12.\nThomas, M., Martin, B., Kowarski, K., Gaudet, B., & Matwin, S. (2019). Marine mammal species classification using convolutional neural networks and a novel acoustic representation. arXiv preprint arXiv:1907.13188.\nUsman, A. M., Ogundile, O. O., & Versfeld, D. J. (2020). Review of automatic detection and classification techniques for cetacean vocalization. IEEE Access, 8, 105181-105206.\nVickers, W., Milner, B., Risch, D., & Lee, R. (2021). Robust North Atlantic right whale detection using deep learning models for denoising. The Journal of the Acoustical Society of America, 149(6), 3797-3812.\n\n\n\n\nImage-based Object Detection\nImage-based object detection methods in bioacoustics leverage spectrograms generated from audio recordings as input for deep learning networks to detect specific calls or signals of interest, such as dolphin whistles. In this process, spectrograms undergo normalization and contrast enhancement to help the model distinguish between calls and background noise. As outlined in Fig. 1, annotations from the audio files are converted into image datastores, which are then used for object detection models in MATLAB. The duration of each spectrogram image is user-defined, with the segment length based on the duration and properties of the target calls of interest. Each spectrogram image is linked to coordinates that identify the location of calls within the image.\nTo help the model understand not just calls but also noise, spectrograms from audio segments without calls are also used. In these cases, random coordinates mimicking call-like features are applied to these images, allowing the model to learn the characteristics of “Noise” signals. This enables the network to better differentiate between calls and non-call elements in the spectrograms.\nFurther refinement of the spectrograms is achieved through contrast-limited adaptive histogram equalization (CLAHE), which enhances the contrast and normalizes the power spectral densities. Additional variability is introduced by augmenting the spectrograms, adjusting parameters like the Fast-Fourier Transform (FFT) size and overlap. The resulting images are then resized to meet the specific input requirements of the object detection networks, based on available computational resources (Table II). This approach ensures the networks are trained on a diverse range of signals, improving their ability to detect calls across different acoustic environments.\n\n\n\nFigure 1: Flow chart of analytical process from raw audio to network selection in DeepAcoustics\n\n\n\n\n\nYOLO Method of Object-Detection\nYOLO (You Only Look Once) is a real-time object detection algorithm that has gained significant attention for its speed and accuracy in identifying objects within images. Unlike traditional methods that use sliding windows or region proposals to detect objects, YOLO treats object detection as a single regression problem, predicting both the class and the bounding box coordinates in a single pass through the network. This streamlined approach allows YOLO to process images much faster, making it well-suited for tasks requiring real-time detection. The algorithm divides the input image into a grid and assigns each grid cell the responsibility of detecting an object if its center falls within that cell. By generating predictions for multiple bounding boxes and their associated confidence scores in one forward pass, YOLO can efficiently detect objects at various scales and positions within the image.\nIn the context of bioacoustics, YOLO can be adapted to detect specific sounds within spectrogram images. These spectrograms serve as visual representations of sound, and YOLO is trained to recognize the distinct patterns that correspond to specific calls. The model’s ability to quickly process entire images and localize calls within them makes it an effective tool for detecting signals of interest in large datasets. YOLO’s grid-based prediction system allows it to handle overlapping calls and background noise, which is critical in bioacoustic environments where signals can be complex and varied. By training the network on annotated spectrograms, YOLO can learn to differentiate between multiple classes of calls and noise in a single pass, allowing for rapid interpretation of large acoustic datasets.",
    "crumbs": [
      "Deep Learning Methods"
    ]
  },
  {
    "objectID": "DA.html#software-requirements",
    "href": "DA.html#software-requirements",
    "title": "DeepAcoustics Guide and Tutorial",
    "section": "Software Requirements:",
    "text": "Software Requirements:\nDeepAcoustics is developed in MATLAB and utilizes the streamlined integration of deep learning modules along with the YOLO (“You Only Look Once”) object detection model, specifically version 4. While this version is not as advanced as Python’s YOLO v9, it offers robust performance for bioacoustic applications. DeepAcoustics requires MATLAB version R2023b or higher, as it leverages the App Designer feature to support the graphical user interface. To run DeepAcoustics, the following are required:\n\nMATLAB version R2023b or higher\nDeep Learning Toolbox\nComputer Vision Toolbox\nSignal Processing Toolbox\nParallel Computing Toolbox (for GPU support)\nStatistics and Machine Learning Toolbox\n\nThese requirements ensure optimal performance and efficient handling of deep learning tasks within the software.",
    "crumbs": [
      "About DeepAcoustics"
    ]
  },
  {
    "objectID": "DA.html#gpu-requirements",
    "href": "DA.html#gpu-requirements",
    "title": "DeepAcoustics Guide and Tutorial",
    "section": "GPU Requirements:",
    "text": "GPU Requirements:\nA GPU (Graphics Processing Unit) is a specialized hardware component designed to handle complex mathematical calculations, especially those related to graphics rendering and parallel processing tasks. While GPUs were originally developed for rendering images and videos in gaming and 3D applications, their architecture has made them highly effective for tasks that involve large-scale matrix and vector operations, which are common in deep learning.\nDeep learning tasks, such as training neural networks, require the processing of vast amounts of data and the performance of many computations simultaneously. These tasks involve operations like multiplying large matrices (weights and inputs), calculating gradients, and updating model parameters across multiple layers of the network. CPUs (Central Processing Units) can handle these tasks but are optimized for sequential processing, meaning they are generally slower when performing the parallelized operations deep learning demands. GPUs, on the other hand, are designed to process thousands of operations in parallel, making them much faster for deep learning tasks. This capability is essential when working with large datasets and complex models, such as convolutional neural networks (CNNs) or object detection models like YOLO, where speed and efficiency are crucial. By using GPUs, deep learning models can be trained faster and more efficiently, significantly reducing the time needed for both training and inference, particularly when working with large-scale or real-time applications.\nA modern GPU with CUDA support is required for training networks in DeepAcoustics, but is not required for using a trained network model to detect target calls in audio using DeepAcoustics.",
    "crumbs": [
      "About DeepAcoustics"
    ]
  },
  {
    "objectID": "cluster.html",
    "href": "cluster.html",
    "title": "cluster",
    "section": "",
    "text": "dfd",
    "crumbs": [
      "Classification",
      "Clustering Data"
    ]
  },
  {
    "objectID": "cluster_evaluate.html",
    "href": "cluster_evaluate.html",
    "title": "cluster_evaluate",
    "section": "",
    "text": "dfd",
    "crumbs": [
      "Classification",
      "Evaluating Clusters"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeepAcoustics_Guide_and_Tutorial",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]